{"cells":[{"cell_type":"markdown","metadata":{"id":"_8fVwOCp9ZXX"},"source":["### Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EH-5hgJ-9ZXd"},"outputs":[],"source":["from datetime import datetime\n","import os, sys\n","import copy\n","\n","import torch\n","import catboost\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","# add my own file with custom utility functions as a module\n","utils_path = os.path.normpath(os.path.abspath(os.path.join(os.path.dirname('__file__'), os.path.pardir)))\n","if utils_path not in sys.path:\n","    sys.path.append(utils_path)\n","\n","import aku_utils as ak\n","\n","import pandas as pd\n","import numpy as np\n","\n","# pandas options\n","pd.options.display.max_columns = 100\n","pd.options.display.max_rows =  200\n","# pd.options.display.max_info_rows = 1690785\n","pd.options.display.max_info_columns = 200\n","pd.options.display.float_format = '{:,.2f}'.format\n","pd.options.display.date_dayfirst = True\n","pd.options.mode.chained_assignment = None"]},{"cell_type":"markdown","metadata":{"id":"v9eotsXt9ZXg"},"source":["### Data overview"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZIgDANcc9ZXh","outputId":"608c95fc-24c6-42d1-fb84-454cdd302c6c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>married</th>\n","      <th>number_of_dependents</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>number_of_referrals</th>\n","      <th>phone_service</th>\n","      <th>multiple_lines</th>\n","      <th>avg_monthly_gb_download</th>\n","      <th>online_security</th>\n","      <th>online_backup</th>\n","      <th>device_protection_plan</th>\n","      <th>premium_tech_support</th>\n","      <th>streaming_tv</th>\n","      <th>unlimited_data</th>\n","      <th>paperless_billing</th>\n","      <th>total_refunds</th>\n","      <th>total_extra_data_charges</th>\n","      <th>total_long_distance_charges</th>\n","      <th>satisfaction_score</th>\n","      <th>churn</th>\n","      <th>cltv</th>\n","      <th>corr_total_charges</th>\n","      <th>contract_one_year</th>\n","      <th>contract_two_year</th>\n","      <th>payment_method_credit_card</th>\n","      <th>payment_method_mailed_check</th>\n","      <th>offer_offer_b</th>\n","      <th>offer_offer_c</th>\n","      <th>offer_offer_d</th>\n","      <th>offer_offer_e</th>\n","      <th>offer_nan</th>\n","      <th>internet_type_dsl</th>\n","      <th>internet_type_fiber_optic</th>\n","      <th>internet_type_nan</th>\n","      <th>streaming_music_or_movies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>78</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34.02</td>\n","      <td>-118.16</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5433</td>\n","      <td>19.82</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>74</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>34.04</td>\n","      <td>-118.19</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.62</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5302</td>\n","      <td>126.66</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>71</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>34.11</td>\n","      <td>-118.23</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>52</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.03</td>\n","      <td>0.00</td>\n","      <td>0.12</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>3179</td>\n","      <td>438.14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>78</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>33.94</td>\n","      <td>-118.33</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.20</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5337</td>\n","      <td>502.90</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>80</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>33.97</td>\n","      <td>-118.02</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.08</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2793</td>\n","      <td>717.04</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7033</th>\n","      <td>0</td>\n","      <td>23</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>33.28</td>\n","      <td>-115.96</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.14</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>5553</td>\n","      <td>865.25</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7034</th>\n","      <td>0</td>\n","      <td>57</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>33.14</td>\n","      <td>-116.97</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.35</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>5191</td>\n","      <td>39.25</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7035</th>\n","      <td>1</td>\n","      <td>63</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>33.04</td>\n","      <td>-115.61</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.51</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4591</td>\n","      <td>875.08</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7036</th>\n","      <td>1</td>\n","      <td>57</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>32.85</td>\n","      <td>-114.85</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2464</td>\n","      <td>498.37</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7037</th>\n","      <td>0</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34.16</td>\n","      <td>-116.43</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3740</td>\n","      <td>148.66</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7038 rows Ã— 37 columns</p>\n","</div>"],"text/plain":["      gender  age  married  number_of_dependents  latitude  longitude  \\\n","0          1   78        0                     0     34.02    -118.16   \n","1          0   74        1                     1     34.04    -118.19   \n","2          1   71        0                     3     34.11    -118.23   \n","3          0   78        1                     1     33.94    -118.33   \n","4          0   80        1                     1     33.97    -118.02   \n","...      ...  ...      ...                   ...       ...        ...   \n","7033       0   23        1                     0     33.28    -115.96   \n","7034       0   57        0                     0     33.14    -116.97   \n","7035       1   63        0                     0     33.04    -115.61   \n","7036       1   57        0                     0     32.85    -114.85   \n","7037       0   62        0                     0     34.16    -116.43   \n","\n","      number_of_referrals  phone_service  multiple_lines  \\\n","0                       0              0               0   \n","1                       1              1               1   \n","2                       0              1               1   \n","3                       1              1               0   \n","4                       1              1               1   \n","...                   ...            ...             ...   \n","7033                    2              1               0   \n","7034                    0              1               0   \n","7035                    0              1               0   \n","7036                    0              1               0   \n","7037                    0              0               0   \n","\n","      avg_monthly_gb_download  online_security  online_backup  \\\n","0                           8                0              0   \n","1                          17                0              1   \n","2                          52                0              0   \n","3                          12                0              1   \n","4                          14                0              0   \n","...                       ...              ...            ...   \n","7033                       53                0              1   \n","7034                        0                0              0   \n","7035                        2                0              0   \n","7036                       13                0              0   \n","7037                       24                0              1   \n","\n","      device_protection_plan  premium_tech_support  streaming_tv  \\\n","0                          1                     0             0   \n","1                          0                     0             0   \n","2                          0                     0             1   \n","3                          1                     0             1   \n","4                          0                     0             0   \n","...                      ...                   ...           ...   \n","7033                       0                     1             1   \n","7034                       0                     0             0   \n","7035                       0                     0             0   \n","7036                       0                     0             1   \n","7037                       1                     1             1   \n","\n","      unlimited_data  paperless_billing  total_refunds  \\\n","0                  0                  1           0.00   \n","1                  1                  1           0.00   \n","2                  1                  1           0.03   \n","3                  1                  1           0.01   \n","4                  1                  1           0.00   \n","...              ...                ...            ...   \n","7033               1                  0           0.00   \n","7034               0                  1           0.00   \n","7035               1                  1           0.01   \n","7036               1                  1           0.02   \n","7037               1                  0           0.05   \n","\n","      total_extra_data_charges  total_long_distance_charges  \\\n","0                         0.50                         0.00   \n","1                         0.00                         0.62   \n","2                         0.00                         0.12   \n","3                         0.00                         0.20   \n","4                         0.00                         0.08   \n","...                        ...                          ...   \n","7033                      0.00                         0.14   \n","7034                      0.00                         0.35   \n","7035                      0.00                         0.51   \n","7036                      0.00                         0.38   \n","7037                      0.00                         0.00   \n","\n","      satisfaction_score  churn  cltv  corr_total_charges  contract_one_year  \\\n","0                      3      1  5433               19.82                  0   \n","1                      3      1  5302              126.66                  0   \n","2                      2      1  3179              438.14                  0   \n","3                      2      1  5337              502.90                  0   \n","4                      2      1  2793              717.04                  0   \n","...                  ...    ...   ...                 ...                ...   \n","7033                   5      0  5553              865.25                  0   \n","7034                   3      0  5191               39.25                  0   \n","7035                   3      0  4591              875.08                  0   \n","7036                   3      0  2464              498.37                  0   \n","7037                   3      0  3740              148.66                  1   \n","\n","      contract_two_year  payment_method_credit_card  \\\n","0                     0                           0   \n","1                     0                           1   \n","2                     0                           0   \n","3                     0                           0   \n","4                     0                           0   \n","...                 ...                         ...   \n","7033                  1                           0   \n","7034                  0                           0   \n","7035                  0                           1   \n","7036                  0                           0   \n","7037                  0                           0   \n","\n","      payment_method_mailed_check  offer_offer_b  offer_offer_c  \\\n","0                               0              0              0   \n","1                               0              0              0   \n","2                               0              0              0   \n","3                               0              0              1   \n","4                               0              0              1   \n","...                           ...            ...            ...   \n","7033                            0              0              0   \n","7034                            1              0              0   \n","7035                            0              0              0   \n","7036                            0              0              0   \n","7037                            0              0              0   \n","\n","      offer_offer_d  offer_offer_e  offer_nan  internet_type_dsl  \\\n","0                 0              0          1                  1   \n","1                 0              1          0                  0   \n","2                 1              0          0                  0   \n","3                 0              0          0                  0   \n","4                 0              0          0                  0   \n","...             ...            ...        ...                ...   \n","7033              0              0          1                  1   \n","7034              0              1          0                  0   \n","7035              0              0          1                  0   \n","7036              0              0          1                  0   \n","7037              0              0          1                  0   \n","\n","      internet_type_fiber_optic  internet_type_nan  streaming_music_or_movies  \n","0                             0                  0                          1  \n","1                             1                  0                          0  \n","2                             1                  0                          1  \n","3                             1                  0                          1  \n","4                             1                  0                          0  \n","...                         ...                ...                        ...  \n","7033                          0                  0                          0  \n","7034                          0                  1                          0  \n","7035                          1                  0                          0  \n","7036                          1                  0                          0  \n","7037                          0                  0                          1  \n","\n","[7038 rows x 37 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(os.path.join(os.path.dirname('__file__'), os.path.pardir, 'data', 'telco_processed_2.csv'))\n","df.head(-5)"]},{"cell_type":"markdown","metadata":{"id":"0eECbLm09j-G"},"source":["# Catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hjU_7bov9ZXj"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GupOGCo9ZXk"},"outputs":[],"source":["model = catboost.CatBoostClassifier(loss_function='Logloss', random_seed=888)\n","\n","# train the model\n","model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4TR7_ke9ZXl","outputId":"f4cf4f00-aa13-4a83-82fd-7c61ff5a65ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0      0.967     0.985     0.976      1031\n","           1      0.958     0.907     0.932       378\n","\n","    accuracy                          0.965      1409\n","   macro avg      0.962     0.946     0.954      1409\n","weighted avg      0.964     0.965     0.964      1409\n","\n"]}],"source":["print(sklearn.metrics.classification_report(y_test, model.predict(X_test), digits=3))"]},{"cell_type":"markdown","metadata":{"id":"UyEK_Fw594tv"},"source":["# NN"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OqPM3pwA9ZXn"},"outputs":[],"source":["def get_data_loaders(df, target, val_rate=.2, batch_size=64):\n","    X_train, X_test, y_train, y_val = train_test_split(df.drop(target, axis=1), df[target], test_size=val_rate)\n","\n","    training_set = torch.utils.data.TensorDataset(\n","        torch.tensor(X_train.values, dtype=torch.float32),\n","        torch.tensor(y_train.values, dtype=torch.float32))\n","\n","    validation_set = torch.utils.data.TensorDataset(\n","        torch.tensor(X_test.values, dtype=torch.float32),\n","        torch.tensor(y_val.values, dtype=torch.float32))\n","\n","    training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n","    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True) # here shuffle is recommended for using early stopping on big batches later\n","    return training_loader, validation_loader\n","\n","training_loader, validation_loader = get_data_loaders(df, 'churn')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# class Trainer():\n","#     def __init__():\n","#         return None\n","\n","\n","#     def train(model,\n","#             loss_fn,\n","#             optimizer,\n","#             training_loader,\n","#             validation_loader,\n","#             *,\n","#             bbatch_size : int | None = 16,\n","#             print_loss : str | None = None,\n","#             graph_loss : str | None = 'bbatch',\n","#             early_stopping : str | None = 'bbatch',\n","#             early_stopping_params : dict | None = None,\n","#             max_epochs : int = 100) -> None:\n","#         '''\n","#         Optional args:\n","#             bbatch_size: size of a big batch (in usual batches, defined by training loader) (16)\n","\n","#             print_loss: print loss (None) ['epoch', 'bbatch', None]\n","#             graph_loss: graph loss ('bbatch') ['epoch', 'bbatch', None]\n","#                 Not recommended to use both\n","\n","#             early_stopping: early stopping done on epochs, big batches or not implemented ('bbatch') ['epoch', 'bbatch', None]\n","#                 It is recommended that early stopping matches loss reporting (big batches/epochs)\n","#             early_stopping_params: dict\n","#                 X\n","#                 path: path to save the model ('models/model' is default and recommended)\n","#                 save: save mode for early stopping ('stop') ['new', 'overwrite', 'stop']\n","#                     'new' - save new model each time improvement is noticed (not recommended),\n","#                     'overwrite' - overwrite model each time improvement is noticed (recommended for smaller NNs)\n","#                     'stop' - do not save at all, simply break training if early stop is triggered (recommended for larger NNs)\n","\n","#             max_epochs: maximum number of epochs to run (100). If reached it will print to stdout, suggesting that the training should be run again.\n","#         '''\n","#         #\n","#         # option validation\n","#         #\n","\n","\n","#         #\n","#         # option processing\n","#         #\n","#         if early_stopping_params is None:\n","#             early_stopping_params = {}\n","\n","#         early_stopping_params.setdefault('path', 'models/model')\n","#         early_stopping_params.setdefault('save', 'stop')\n","\n","#         needs_epoch_val_loss = (print_loss == 'epoch') or (graph_loss == 'epoch')\n","\n","#         # option processing for the _train_epoch()\n","#         # all the things like early stopping is done per big batch inside _train_epoch()\n","#         # thats why params below are named 'bbatch_...'\n","#         bbatch_print_loss = print_loss == 'bbatch'\n","#         bbatch_graph_loss = graph_loss == 'bbatch'\n","#         bbatch_early_stopping = early_stopping == 'bbatch'\n","\n","#         #\n","#         # start\n","#         #\n","\n","#         min_val_loss = 1e9\n","#         for epoch in range(max_epochs):\n","\n","#             train_loss = _train_epoch(model, loss_fn, optimizer, training_loader, validation_loader,\n","#                                         bbatch_size = bbatch_size,\n","#                                         print_loss = bbatch_print_loss,\n","#                                         graph_loss = bbatch_graph_loss,\n","#                                         early_stopping = bbatch_early_stopping,\n","#                                         early_stopping_params = early_stopping_params)\n","\n","#             if needs_epoch_val_loss:\n","#                 pass # val_loss = _validate_epoch(model, validation_loader)\n","\n","#             if print_loss == 'epoch':\n","#                 pass\n","\n","#             if graph_loss == 'epoch':\n","#                 pass\n","\n","#         return None\n","\n","\n","#     def _train_epoch(model,\n","#                     loss_fn,\n","#                     optimizer,\n","#                     training_loader,\n","#                     validation_loader,\n","#                     *,\n","#                     bbatch_size : int | None = 16,\n","#                     print_loss : bool,\n","#                     graph_loss : bool,\n","#                     early_stopping : bool,\n","#                     early_stopping_params : dict | None = None) -> float:\n","\n","#         epoch_loss = 0.\n","#         prev_value_epoch_loss = 0. # for training big batch loss reporting\n","\n","#         for batch_index, (inputs, labels) in enumerate(training_loader):\n","\n","#             # training stuff\n","#             optimizer.zero_grad()\n","\n","#             outputs = model(inputs)\n","#             loss = loss_fn(outputs, labels.unsqueeze(1))\n","\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             # sum loss for reporting\n","#             epoch_loss += loss.item()\n","\n","#             if batch_index % bbatch_size == bbatch_size - 1:\n","\n","#                 # we use this to extract only the loss attributed to this big batch\n","#                 # we do not track big batch explicitly, because we do that for epoch loss\n","#                 training_big_batch_loss = epoch_loss - prev_value_epoch_loss\n","#                 prev_value_epoch_loss = copy.deepcopy(epoch_loss)\n","\n","#                 validation_big_batch_loss = ...\n","\n","#                 # early stopping\n","\n","#                 # plot training_big_batch_loss and validation_big_batch_loss\n","\n","#         epoch_loss = epoch_loss / (batch_index + 1)\n","#         return epoch_loss"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8pzGmqOU9ZXn"},"outputs":[],"source":["class NeuralNetwork(torch.nn.Module):\n","    def __init__(self, n_features_in : int):\n","        super().__init__()\n","        self.linear_relu_stack = torch.nn.Sequential(\n","            torch.nn.Linear(n_features_in, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 1),\n","            torch.nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.linear_relu_stack(x)\n","        return x\n","\n","model = NeuralNetwork(df.shape[1]-1)\n","\n","loss_fn = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["45.44264409247409\n","1.7010993187347154\n","0.5583288880546441\n","0.5221364672264356\n","0.5192134236351827\n","0.507265346606126\n","0.5030971264906143\n","0.5086156056168374\n","0.5054331324743421\n","0.5055928290560004\n","0.5013445826728692\n","0.4903987883851769\n","0.4814265225543065\n","0.4967635247144806\n","0.4904876954100105\n","0.47804487588700284\n","0.47830919737226507\n","0.4715887324863605\n","0.4798377594921026\n","0.46283281468943266\n"]}],"source":["class Trainer():\n","    def __init__(self,\n","                 model,\n","                 loss_fn,\n","                 optimizer,\n","                 training_loader,\n","                 validation_loader,\n","                 *,\n","                 bbatch_size : int | None = 16,\n","                 val_bbatch_size : int | None = 16,\n","                 print_loss : str | None = None,\n","                 graph_loss : str | None = 'bbatch',\n","                 early_stopping : str | None = 'bbatch',\n","                 early_stopping_params : dict | None = None,\n","                 max_epochs : int = 100) -> None:\n","        '''\n","        Loss tracking is done by storing the losses as pandas dataframe with columns:\n","            epoch\n","            big train batch index\n","            big train batch loss\n","            big val batch loss\n","\n","        Loss tracking for user is done by 1) printing 2) graphing 3) Nothing.\n","\n","        Loss tracking is done either on big batches or epochs.\n","\n","        Early stopping is done either on big batches or epochs.\n","\n","        Optional args:\n","            bbatch_size: size of a big batch (in usual batches, defined by training loader) (16)\n","            val_bbatch_size: same for validation. Losses for big val batches will be calculated after a big train batch has run\n","\n","            print_loss: print loss (None) ['epoch', 'bbatch', None]\n","            graph_loss: graph loss ('bbatch') ['epoch', 'bbatch', None]\n","                Not recommended to use both\n","\n","            early_stopping: early stopping done on epochs, big batches or not implemented ('bbatch') ['epoch', 'bbatch', None]\n","                It is recommended that early stopping matches loss reporting (big batches/epochs)\n","            early_stopping_params: dict\n","                X\n","                path: path to save the model ('models/model' is default and recommended)\n","                save: save mode for early stopping ('stop') ['new', 'overwrite', 'stop']\n","                    'new' - save new model each time improvement is noticed (not recommended),\n","                    'overwrite' - overwrite model each time improvement is noticed (recommended for smaller NNs)\n","                    'stop' - do not save at all, simply break training if early stop is triggered (recommended for larger NNs)\n","\n","            max_epochs: maximum number of epochs to run (100)\n","        '''\n","        #\n","        # option validation\n","        #\n","\n","\n","        #\n","        # option processing\n","        #\n","        if early_stopping_params is None:\n","            early_stopping_params = {}\n","\n","        early_stopping_params.setdefault('path', 'models/model')\n","        early_stopping_params.setdefault('save', 'stop')\n","\n","        self.model = model\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","        self.training_loader = training_loader\n","        self.validation_loader = validation_loader\n","        self.bbatch_size = bbatch_size\n","        self.val_bbatch_size = val_bbatch_size\n","        self.print_loss = print_loss\n","        self.graph_loss = graph_loss\n","        self.early_stopping = early_stopping\n","        self.early_stopping_params = early_stopping_params\n","        self.max_epochs = max_epochs\n","\n","        self.cur_epoch = None\n","        self.train_epoch_losses = []\n","        self.val_epoch_losses = []\n","        self.train_bbatch_losses = []\n","        self.val_bbatch_losses = []\n","        self.epoch_loss_df = pd.DataFrame()\n","        self.bbatch_loss_df = pd.DataFrame()\n","        return None\n","\n","\n","    def train(self) -> None:\n","        for epoch in range(self.max_epochs):\n","            self.cur_epoch = epoch\n","\n","            if self.print_loss:\n","                # print epoch title\n","                pass\n","\n","            train_loss = self._train_epoch()\n","            self.train_epoch_losses.append(train_loss)\n","            print(train_loss)\n","            if self.print_loss == 'epoch' or self.graph_loss == 'epoch':\n","                pass # val_loss = _validate_epoch()\n","\n","            if self.print_loss == 'epoch':\n","                # print epoch train and val losses \n","                pass\n","\n","            if self.graph_loss == 'epoch':\n","                pass\n","            \n","            # if early stop has activated: stop training and print it out\n","\n","        # if all epochs have run, print that\n","        return None\n","\n","\n","    def _train_epoch(self) -> float:\n","        '''\n","        Train for epoch, return mean training loss\n","        '''\n","        epoch_loss = 0.\n","        prev_value_epoch_loss = 0. # for training big batch loss reporting\n","\n","        for batch_index, (inputs, labels) in enumerate(self.training_loader):\n","\n","            # training stuff\n","            self.model.train()\n","            self.optimizer.zero_grad()\n","\n","            outputs = self.model(inputs)\n","            \n","            loss = self.loss_fn(outputs, labels.unsqueeze(1))\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            # sum loss for reporting\n","            epoch_loss += loss.item()\n","\n","            if self.bbatch_size is not None and batch_index % self.bbatch_size == self.bbatch_size - 1:\n","\n","                # we use this to extract only the loss attributed to this big batch\n","                # we do not track big batch explicitly, because we do that for epoch loss\n","                training_bbatch_loss = (epoch_loss - prev_value_epoch_loss) / self.bbatch_size\n","                prev_value_epoch_loss = copy.deepcopy(epoch_loss)\n","                self.train_bbatch_losses.append(training_bbatch_loss)\n","\n","                val_bbatch_loss = self._get_val_loss(on='bbatch')\n","                self.val_bbatch_losses.append(val_bbatch_loss)\n","\n","                # early stopping\n","\n","                # plot training_big_batch_loss and validation_big_batch_loss\n","\n","        epoch_loss = epoch_loss / (batch_index + 1)\n","        return epoch_loss\n","\n","\n","    def _get_val_loss(self, on : str) -> float:\n","        '''\n","        Function sets the modes (eval, train) itself\n","        '''\n","        val_loss = 0.\n","        self.model.eval()\n","\n","        with torch.no_grad():\n","            for batch_index, (inputs, labels) in enumerate(self.validation_loader):\n","                outputs = self.model(inputs)\n","                loss = self.loss_fn(outputs, labels.unsqueeze(1))\n","                val_loss += loss.item()\n","\n","                if on == 'bbatch' and batch_index % self.val_bbatch_size == self.val_bbatch_size - 1:\n","                    break\n","        \n","        val_loss = val_loss / (batch_index + 1)\n","        return val_loss\n","\n","\n","Trainer(model, loss_fn, optimizer, training_loader, validation_loader,\n","        bbatch_size=32, val_bbatch_size=32, max_epochs=20).train()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
